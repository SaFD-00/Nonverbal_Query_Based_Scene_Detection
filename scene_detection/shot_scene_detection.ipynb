{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shot-Scene Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 설정사항\n",
    "1. ShotDetector Debug안에 video 폴더가 존재해야함.\n",
    "2. 아래의 video 경로를 video가 있는 폴더로 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene detection module 있는 곳으로 이동\n",
    "os.chdir(r\"E:\\project_backup\\ShotDetector\\x64\\Debug\")\n",
    "videos = os.listdir(r\"E:\\project_backup\\ShotDetector\\x64\\Debug\\news\")\n",
    "\n",
    "## config 설정\n",
    "\n",
    "buffer_frame_size = 3\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 잘 받아오는지 확인용\n",
    "videos[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자가 존재할시 읽지 못하는 상황 발생하여 특수문자, 띄어쓰기를 정규식으로 제거.\n",
    "\n",
    "base_dir = r\"E:\\project_backup\\ShotDetector\\x64\\Debug\\news\\\\\"\n",
    "for i in tqdm(videos):\n",
    "    os.rename(base_dir + i , re.sub(' |’', '_', base_dir + i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm(videos) path를 바탕으로 shot detection 수행함.\n",
    "\n",
    "for i in tqdm(videos):\n",
    "    #print(path)\n",
    "    path = \"E:\\\\project_backup\\\\ShotDetector\\\\x64\\\\Debug\\\\news\\\\\" + i\n",
    "    os.system(f'ilsd {path} {buffer_frame_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "- qvhilight dataset(shot detection 후)\n",
    "- youtube dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shot-Scene Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (1) shot detection 결과 전처리하기\n",
    "video -> shot detection -> middle shot.txt 파일  \n",
    "video의 shot 별 middle frame을 inception v3 모델에 돌려서 특징 추출  \n",
    "특징 추출 -> h5파일로 만들어서 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Module 볼러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL\n",
    "import re\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import cv2\n",
    "import av\n",
    "import pickle\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.io import read_video\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data 폴더 안에 있는 파일들 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"D:\\youtube_data\" # 폴더 경로 지정\n",
    "file_list = os.listdir(folder_path) # 폴더 안에 있는 파일 목록\n",
    "video_list = [file_name for file_name in file_list if file_name.endswith('.mp4')] # 비디오 목록 리스트\n",
    "shot_list = [file_name for file_name in file_list if file_name.endswith('.txt')] # 샷 리스트 - 순서대로 저장 0-0\n",
    "print(shot_list)\n",
    "print(len(shot_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 별로 middle shot list 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_shots_dict = {}\n",
    "for idx, n in enumerate(shot_list):\n",
    "    shot_filepath = folder_path + '\\\\' + n\n",
    "    shot = pd.read_csv(shot_filepath, header = None, delimiter='\\t') # 처리\n",
    "    shot['mid'] = round((shot[0] + shot[1])/2).astype(int) # 중간 frame 뽑아와\n",
    "    shot['frame_cnt'] = shot[1]-shot[0] # frame 개수는 이거\n",
    "    middle_shots = shot['mid'].tolist() # middle shot list\n",
    "    middle_shots_dict[idx] = middle_shots\n",
    "    #print(len(middle_shots))\n",
    "\n",
    "print(middle_shots_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_video_frame 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame(video_path, frame_number):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not video.isOpened():\n",
    "        raise Exception(\"unable open the video\")\n",
    "\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_number > total_frames:\n",
    "        raise Exception(\"FRAME NUMBER >>>> TOTAL FRAME!@!!\")\n",
    "\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    if not ret:\n",
    "        raise Exception(\"프레임을 가져올 수 없음\")\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 안에 있는 img data 불러오기\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.x_data = imgs\n",
    "        self.transforms = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = cv2.resize(self.x_data[idx] , dsize=(299,299))\n",
    "        x = cv2.normalize(x, None, 0, 1, cv2.NORM_MINMAX) # minmax_norm\n",
    "        x = torch.FloatTensor(x)\n",
    "        x = x.permute((2,0,1))\n",
    "        tf = transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]) # 이미지 정규화\n",
    "        x = tf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video 별 feature 추출 -> h5 파일로 저장\n",
    "imgs = []\n",
    "feature_vectors =[] # 저장할라고 빈 리스트로 생성\n",
    "\n",
    "for k,v in middle_shots_dict.items(): # 저장되어있는 middel_Shot # key 만큼 돌고,\n",
    "\n",
    "    video_path = folder_path + '\\\\' + video_list[k]\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    for middle_shot_numbers in v: # v list 안에 하나하나 돌아서 -> middel shot number 불러옴\n",
    "        img = get_video_frame(video_path, middle_shot_numbers) # 중간 프레임 뽑아와\n",
    "        imgs.append(img)\n",
    "\n",
    "    dataset = MyDataset(imgs=imgs) # dataset 가져오면서 전처리해주기\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "    model.eval()\n",
    "    model.fc = torch.nn.Identity() # 입력 그대로 출력으로 반환 (항등함수)\n",
    "\n",
    "    # GPU를 사용할 경우\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 특성 벡터를 추출\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # forward  패스 # model의 출력을 계산하는 부분\n",
    "        with torch.no_grad():\n",
    "            features = model(images)\n",
    "\n",
    "        feature_vectors.append(features.squeeze().cpu().numpy())\n",
    "\n",
    "    # feature vector 저장\n",
    "    for idx,features in enumerate(feature_vectors):\n",
    "        features_np = np.array(features)\n",
    "        print(f\"frame {idx+1} features:\", features_np)\n",
    "\n",
    "    directory = \"D:\\Pycharm project\\LearnableOSG\\h5\\youtube_h5\"\n",
    "    filename = f'{re.split(\".mp4\", video_list[k])[0]}.hdf5'\n",
    "\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    file = h5py.File(os.path.join(directory,filename), \"w\") # h5 파일로 저장\n",
    "\n",
    "    feature_vectors_np = np.stack(feature_vectors, axis=0)\n",
    "\n",
    "    # 데이터셋 생성 및 데이터 저장\n",
    "    dataset_name = \"x\"\n",
    "    dataset = file.create_dataset(dataset_name, data=feature_vectors_np)\n",
    "\n",
    "    # 각 feature vector의 이름을 속성으로 추가\n",
    "    for idx, _ in enumerate(feature_vectors):\n",
    "        dataset.attrs[str(idx)] = str(idx)\n",
    "\n",
    "    # HDF5 파일 닫기\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    #초기화\n",
    "    imgs = []\n",
    "    feature_vectors = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2) scene detection 수행\n",
    "- h5 file(x) -> scene detection -> h5 file(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scene detection model module화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_loader(train).py\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import re\n",
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "import h5py\n",
    "\n",
    "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
    "\n",
    "\n",
    "\n",
    "class OSG_VSD_DATASET(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_h5, device):\n",
    "        self.path_to_h5 = path_to_h5\n",
    "        self.device = device\n",
    "        self.num_of_h5 = 3 # 51개 (hdf5 파일의 갯수만큼 지정해주기)\n",
    "\n",
    "\n",
    "    def __len__ (self):\n",
    "        return self.num_of_h5 # 파일 갯수만큼 길이 읽어오기\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        the_file = h5py.File(self.path_to_h5+str(idx)+'.hdf5', 'r')\n",
    "        return torch.tensor(the_file['x'], dtype=torch.float, device=self.device), torch.tensor(the_file['t'], dtype=torch.float, device=self.device)\n",
    "\n",
    "\n",
    "def my_collate_old(batch):\n",
    "    data = [item['x'] for item in batch]\n",
    "    target = [item['t'] for item in batch]\n",
    "    return [data, target]\n",
    "\n",
    "\n",
    "my_collate_err_msg_format = (\n",
    "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
    "\"dicts or lists; found {}\")\n",
    "\n",
    "def my_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        max_length = max([x.shape[0] for x in batch])\n",
    "        numel = max_length*len(batch)\n",
    "        storage = elem.storage()._new_shared(numel)\n",
    "        out = elem.new(storage)\n",
    "        return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=-1)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(my_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return my_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: my_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(my_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [my_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(my_collate_err_msg_format.format(elem_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OptimalSequentialGrouping.py \n",
    "import numpy as np\n",
    "\n",
    "class OptimalSequentialGrouping(object):\n",
    "    \"\"\"\n",
    "    Segmentation class which performs optimal sequential grouping (OSG)\n",
    "    \"\"\"\n",
    "\n",
    "    def ismember(self, A, B):\n",
    "        '''\n",
    "        배열 A의 각 요소가 배열 B에 포함되는지 여부를 반환함. A,B 모두 Numpy 배열로 입력\n",
    "        '''\n",
    "        return np.asarray([np.sum(a == B) for a in A]) # A의 요소가 B에 포함되면 1을 반환, 아니면 0을 반환\n",
    "\n",
    "    def blockDivideDSum(self, D, K):\n",
    "        \"\"\"\n",
    "        This method performs OSG using the sum objective function.\n",
    "        :param D: input distance matrix\n",
    "        :param K: number of blocks to divide the distance matrix into\n",
    "        :return: the last index of each block\n",
    "        \"\"\"\n",
    "\n",
    "        N = D.shape[0] # D의 행 크기\n",
    "\n",
    "        # 오류 처리\n",
    "        if N < 1 or K < 1 or N != D.shape[1]:\n",
    "            print(\"Error: Problem with input.\")\n",
    "            return []\n",
    "\n",
    "        if K > N:\n",
    "            print(\"Warning: More scenes than shots. Returning shot boundaries.\")\n",
    "            return np.arrange(1, N + 1)\n",
    "\n",
    "        if K == 1:\n",
    "            return [N - 1]\n",
    "\n",
    "        D_sum = OptimalSequentialGrouping.calcDSum(self, D) # calcDSum 메서드를 호출해서 D_sum을 계산한다. -> 주어진 D 배열을 기반으로, 거리합을 계산하는 과정\n",
    "\n",
    "        C = np.zeros((N, K)) # 크기가 (N,K)인 0으로 채워진 배열\n",
    "        I = np.zeros((N, K)) # 크기가 (N,K)인 0으로 채워진 배열 - 알고리즘의 중간 계산 결과 저장(중간 배열 초기화)\n",
    "\n",
    "        # initialization\n",
    "        for nn in range(0, N):\n",
    "            C[nn, 0] = D_sum[nn, N - 1] # C 배열의 첫번째 열을 채우는 것\n",
    "            I[nn, 0] = N - 1 # I 배열의 첫번째 열을 채우는 것\n",
    "\n",
    "        # fill the rest\n",
    "        for kk in range(1, K): # 알고리즘의 두 번째 열부터 마지막 열까지 업데이트\n",
    "            for nn in range(0, N - kk): # 현재 처리 중인 블록의 첫번째 행부터 마지막 행까지 업데이트\n",
    "                # T will hold the vector in which we're searching for a minimum\n",
    "                T = np.transpose(D_sum[nn, nn:N - kk]) + C[nn + 1:N - kk + 1, kk - 1]\n",
    "                I[nn, kk] = np.argmin(T)\n",
    "                C[nn, kk] = T[int(I[nn, kk])]\n",
    "                I[nn, kk] = I[nn, kk] + nn\n",
    "\n",
    "        # prepare returned boundaries\n",
    "        boundary_locations = np.zeros(K)\n",
    "        the_prev = -1\n",
    "        for kk in range(0, K):\n",
    "            boundary_locations[kk] = I[int(the_prev + 1), K - kk - 1]\n",
    "            the_prev = boundary_locations[kk]\n",
    "\n",
    "        if the_prev != N - 1:\n",
    "            print(\"Warning: Encountered an unknown problem.\")\n",
    "\n",
    "        return boundary_locations\n",
    "\n",
    "    def blockDivide2DSum(self, D1, D2, K, metric='average'):\n",
    "        \"\"\"\n",
    "        This method performs multimodal OSG using the sum objective function.\n",
    "        :param D1: input distance matrix 1\n",
    "        :param D2: input distance matrix 2\n",
    "        :param K: number of blocks to divide the distance matrix into\n",
    "        :param metric: method to decide on joint outcome (default 'average', other options 'min', 'max')\n",
    "        :return: the last index of each block\n",
    "        \"\"\"\n",
    "\n",
    "        N = D1.shape[0]\n",
    "\n",
    "        if N < 1 or K < 1 or N != D1.shape[1] or N != D2.shape[0] or N != D2.shape[1]:\n",
    "            print(\"Error: Problem with input.\")\n",
    "            return []\n",
    "\n",
    "        if K > N:\n",
    "            print(\"Warning: More scenes than shots. Returning shot boundaries.\")\n",
    "            return np.arrange(1, N + 1)\n",
    "\n",
    "        if K == 1:\n",
    "            return [N - 1]\n",
    "\n",
    "        D1_sum = OptimalSequentialGrouping.calcDSum(self, D1)\n",
    "        D2_sum = OptimalSequentialGrouping.calcDSum(self, D2)\n",
    "\n",
    "        C1 = np.zeros((N, K))\n",
    "        C2 = np.zeros((N, K))\n",
    "        I = np.zeros((N, K))\n",
    "\n",
    "        # initialization\n",
    "        for nn in range(0, N):\n",
    "            C1[nn, 0] = D1_sum[nn, N - 1]\n",
    "            C2[nn, 0] = D2_sum[nn, N - 1]\n",
    "            I[nn, 0] = N - 1\n",
    "\n",
    "        # fill the rest\n",
    "        for kk in range(1, K):\n",
    "            for nn in range(0, N - kk):\n",
    "                # T will hold the vector in which we're searching for a minimum\n",
    "                T1 = np.transpose(D1_sum[nn, nn:N - kk]) + C1[nn + 1:N - kk + 1, kk - 1]\n",
    "                C1[nn, kk] = np.min(T1)\n",
    "                T2 = np.transpose(D2_sum[nn, nn:N - kk]) + C2[nn + 1:N - kk + 1, kk - 1]\n",
    "                C2[nn, kk] = np.min(T2)\n",
    "\n",
    "                if T1.size < 2:\n",
    "                    I[nn, kk] = nn\n",
    "                else:\n",
    "                    if metric == 'average':\n",
    "                        E = (T1 - np.mean(T1)) / np.std(T1) + (T2 - np.mean(T2)) / np.std(T2)\n",
    "                    elif metric == 'min':\n",
    "                        E = np.minimum((T1 - np.mean(T1)) / np.std(T1), (T2 - np.mean(T2)) / np.std(T2))\n",
    "                    elif metric == 'max':\n",
    "                        E = np.maximum((T1 - np.mean(T1)) / np.std(T1), (T2 - np.mean(T2)) / np.std(T2))\n",
    "                    else:\n",
    "                        print('Error. Unrecognized metric: ' + metric + '. Performing average.')\n",
    "                        E = (T1 - np.mean(T1)) / np.std(T1) + (T2 - np.mean(T2)) / np.std(T2)\n",
    "\n",
    "                    I[nn, kk] = np.argmin(E) + nn\n",
    "\n",
    "        # prepare returned boundaries\n",
    "        boundary_locations = np.zeros(K)\n",
    "        the_prev = -1\n",
    "        for kk in range(0, K):\n",
    "            boundary_locations[kk] = I[int(the_prev + 1), K - kk - 1]\n",
    "            the_prev = boundary_locations[kk]\n",
    "\n",
    "        if the_prev != N - 1:\n",
    "            print(\"Warning: Encountered an unknown problem.\")\n",
    "\n",
    "        return boundary_locations\n",
    "\n",
    "    def calcDSum(self, D):\n",
    "        \"\"\"\n",
    "        This method calculates the intermediate sums of D.\n",
    "        :param D: input distance matrix\n",
    "        :return: D_sum. A matrix which the value at (i,j) is the sum of the values in D from ii to jj\n",
    "        \"\"\"\n",
    "\n",
    "        N = D.shape[0]\n",
    "\n",
    "        D_sum = np.zeros((N, N))\n",
    "\n",
    "        for oo in range(1, N):\n",
    "            for ii in range(0, N - oo):\n",
    "                D_sum[ii, ii + oo] = 2 * D[ii, ii + oo] + D_sum[ii, ii + oo - 1] + D_sum[ii + 1, ii + oo] - D_sum[\n",
    "                    ii + 1, ii + oo - 1]\n",
    "                D_sum[ii + oo, ii] = D_sum[ii, ii + oo]\n",
    "\n",
    "        return D_sum\n",
    "\n",
    "    def DFromX(self, x, dist_type='euclidean'):\n",
    "        if dist_type == 'euclidean':\n",
    "            return np.linalg.norm(x[:, None] - x, axis=2, ord=2)\n",
    "        elif dist_type == 'cosine':\n",
    "            x_corr = np.matmul(x, x.T)\n",
    "            x_square = np.diag(x_corr)\n",
    "            x_square_rows = np.repeat(x_square[:, None], x.shape[0], axis=1)\n",
    "            x_square_cols = x_square_rows.T\n",
    "            return (1.0 - x_corr / (np.sqrt(x_square_rows * x_square_cols) + 1e-8)) / 2.0\n",
    "        else:\n",
    "            print(\"unrecognized distance type\")\n",
    "            return None\n",
    "\n",
    "    def PR(self, input, ground_truth):\n",
    "        TP = np.intersect1d(input,ground_truth).size\n",
    "        return TP / input.size, TP / ground_truth.size\n",
    "\n",
    "    def FCO(self, input, ground_truth):\n",
    "        num_shots = int(ground_truth[-1])+1\n",
    "        num_scenes = ground_truth.size\n",
    "\n",
    "        gt_scene_numbering = np.zeros(num_shots)\n",
    "        new_scene_numbering = np.zeros(num_shots)\n",
    "        gt_scene_number = 0\n",
    "        new_scene_number = 0\n",
    "        for shot_ind in range(num_shots):\n",
    "            gt_scene_numbering[shot_ind] = gt_scene_number\n",
    "            new_scene_numbering[shot_ind] = new_scene_number\n",
    "            if shot_ind == ground_truth[gt_scene_number]:\n",
    "                gt_scene_number += 1\n",
    "            if shot_ind == input[new_scene_number]:\n",
    "                new_scene_number += 1\n",
    "\n",
    "        Ct = np.zeros(num_scenes)\n",
    "        C = 0\n",
    "        Ot = np.zeros(num_scenes)\n",
    "        O = 0\n",
    "\n",
    "        for scene_num in range(num_scenes):\n",
    "            (_, _, counts) = np.unique(new_scene_numbering[gt_scene_numbering == scene_num], return_index=True, return_counts=True)\n",
    "            if counts.size == 0:\n",
    "                freq = 0\n",
    "            else:\n",
    "                freq = np.max(counts)\n",
    "            Ct[scene_num] = freq/np.sum(gt_scene_numbering == scene_num)\n",
    "            C += Ct[scene_num] * np.sum(gt_scene_numbering == scene_num)\n",
    "            Ot[scene_num] = sum(self.ismember(new_scene_numbering,np.unique(new_scene_numbering[gt_scene_numbering == scene_num])) & self.ismember(gt_scene_numbering, [scene_num-1, scene_num+1]))/np.sum(self.ismember(gt_scene_numbering, [scene_num-1, scene_num+1]))\n",
    "            O += Ot[scene_num] * np.sum(gt_scene_numbering == scene_num)\n",
    "\n",
    "        C = C/num_shots\n",
    "        O = O/num_shots\n",
    "\n",
    "        F = 2*C*(1-O)/(C+1-O)\n",
    "\n",
    "        return F, C, O\n",
    "\n",
    "# input_D를 기반으로 장면의 개수(K)를 추정하는 함수.\n",
    "    def estimate_num_scenes(self, input_D, D_t):\n",
    "        if D_t is None: # D_t가 존재하지 않으면,\n",
    "            D = input_D / input_D.max() # input_D를 최댓값으로 나눠서 정규화된 거리 행렬 'D'를 계산한다.\n",
    "        else: # 그렇지 않으면, (D_t가 존재한다면,)\n",
    "            D_t = np.where(input_D > 0.3, 1, 0)\n",
    "            #sns.heatmap(D_t)\n",
    "            #plt.show(D_t)\n",
    "            D = np.multiply(D_t, input_D / input_D.max()) # D_t와 input_D를 곱한 후, input_D의 최댓값으로 나누어 'D'를 계산한다.\n",
    "            #sns.heatmap(D)\n",
    "            #plt.show(D)\n",
    "        S = np.linalg.svd(D,compute_uv=False) # D의 SVD(특잇값 분해)를 수행하고, ->  S : singular values of D (특이값)\n",
    "        the_graph = np.log(S[S>1.1]) # 특이값 중 1보다 큰 값에 대한 로그를 취한 배열(the_graph)를 생성한다. - 특이값은 기하급수적으로 감소하기 때문에 1보다 큰 특이값의 log를 계산한다.\n",
    "        graph_length = len(the_graph) # the_graph의 길이 저장\n",
    "        if graph_length < 2: # 길이가 2보다 작으면,\n",
    "            return 1 # 장면의 개수는 1로 반환\n",
    "        b = [graph_length - 1, the_graph[-1] - the_graph[0]] # 벡터 b를 계산 b = [그래프의 마지막 인덱스, (리스트의 마지막 값 - 첫번째 값의 차이)]\n",
    "        b_hat = b / np.sqrt(b[0]**2 + b[1]**2) # b를 벡터의 크기로 나눈 정규화된 벡터 ?\n",
    "        p = np.transpose(np.vstack((np.arange(0,graph_length),the_graph-the_graph[0]))) # (0, graph_length) 범위의 정수와 the_graph-the_graph[0]로 구성된 배열\n",
    "        the_subt = p - np.transpose(np.vstack(((p*np.tile(b_hat, [graph_length, 1])).sum(1), (p*np.tile(b_hat, [graph_length, 1])).sum(1)))) * np.tile(b_hat,[graph_length, 1])\n",
    "        the_dist = np.sqrt(the_subt[:, 0]**2 + the_subt[:, 1]**2) # the_subt의 첫번째 열과 두 번째 열의 제곱을 더한 후, 제곱근을 취한 결과\n",
    "        the_elbow = np.nonzero(the_dist==np.max(the_dist))[0][0] # 최댓값과 동일한 값의 인덱스를 찾고, 첫번째 인덱스를 the_elbow로 저장\n",
    "        return the_elbow + 1 # 장면의 개수를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OSG_SVD.py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def my_Tloss(input_prediction_stack,input_groundtruth_stack, device=torch.device(\"cuda\")):\n",
    "    total_loss = 0\n",
    "    input_prediction_list = input_prediction_stack.unbind()\n",
    "    input_groundtruth_list = input_groundtruth_stack.unbind()\n",
    "    for the_ind in range(len(input_prediction_list)):\n",
    "        input_prediction, input_groundtruth = input_prediction_list[the_ind], input_groundtruth_list[the_ind]\n",
    "        T_pred = torch.masked_select(input_prediction, ~torch.isnan(input_prediction))\n",
    "        t = torch.masked_select(input_groundtruth, ~torch.isnan(input_groundtruth))\n",
    "        T_gt = torch.zeros(int(t[-1]), device=device)\n",
    "        for scene_end in t[:-1]:\n",
    "            T_gt[int(scene_end)] = 1\n",
    "        total_loss = total_loss + torch.masked_select(T_pred[:-1], T_gt.ge(1)).clamp(min=1e-3).log().neg().sum()\n",
    "    return total_loss\n",
    "\n",
    "class DIST(torch.nn.Module):\n",
    "    def __init__(self, feature_sizes, BN=False, DO=0.0, dist_type='EMBEDDING', dist_metric='cosine', device=torch.device(\"cuda\")):\n",
    "        super(DIST, self).__init__()\n",
    "        self.device = device\n",
    "        self.feature_sizes =feature_sizes\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(DO)\n",
    "        self.network = torch.nn.Sequential()\n",
    "        for layer_num in range(0, len(feature_sizes)-1):\n",
    "            self.network.add_module('FC'+str(layer_num),torch.nn.Linear(feature_sizes[layer_num], feature_sizes[layer_num+1]))\n",
    "            if isinstance(BN, list):\n",
    "                if BN[layer_num]:\n",
    "                    self.network.add_module('BN'+str(layer_num),torch.nn.BatchNorm1d(feature_sizes[layer_num+1]))\n",
    "            else:\n",
    "                if BN:\n",
    "                    self.network.add_module('BN'+str(layer_num),torch.nn.BatchNorm1d(feature_sizes[layer_num+1]))\n",
    "            if layer_num < len(feature_sizes)-2:\n",
    "                self.network.add_module('ACT'+str(layer_num),self.activation)\n",
    "                if DO > 0.0:\n",
    "                    self.network.add_module('DO' + str(layer_num),self.dropout)\n",
    "        self.dist_type = dist_type\n",
    "        self.dist_metric = dist_metric\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        if list(input_x.shape)[0] > 1:\n",
    "            print('Warning - expected batch size 1')\n",
    "        x = input_x.squeeze(0)\n",
    "\n",
    "        if self.dist_type == 'DIST':\n",
    "            x_new = torch.cat((x.repeat(x.shape[0],1),x.repeat(1,x.shape[0]).view(x.shape[0]*x.shape[0],-1)),1)\n",
    "        elif self.dist_type == 'EMBEDDING':\n",
    "            x_new = x\n",
    "        else:\n",
    "            print('Warning - unrecognized dist_type. Performing EMBEDDING.')\n",
    "            x_new = x\n",
    "\n",
    "        x_new = self.network(x_new)\n",
    "\n",
    "        if self.dist_type=='EMBEDDING':\n",
    "            if self.dist_metric=='cosine':\n",
    "                x_new_corr = x_new.matmul(x_new.t())\n",
    "                x_new_square = torch.masked_select(x_new_corr, torch.eye(x_new.shape[0], device=self.device).ge(1))\n",
    "                x_new_square_rows = x_new_square[:, None].repeat(1, x_new.shape[0])\n",
    "                x_new_square_cols = x_new_square.t().repeat(x_new.shape[0], 1)\n",
    "                D = (1.0 - x_new_corr / (x_new_square_rows * x_new_square_cols).clamp(min=1e-8).sqrt()) / 2.0\n",
    "            elif self.dist_metric=='euclidean':\n",
    "                D = torch.norm(x_new[:, None] - x_new, dim=2, p=2)\n",
    "            else:\n",
    "                print('Warning - unrecognized dist_metric. Performing euclidean.')\n",
    "                D = torch.norm(x_new[:, None] - x_new, dim=2, p=2)\n",
    "        elif self.dist_type=='DIST':\n",
    "            D = x_new.view(x.shape[0],-1)\n",
    "\n",
    "        D.unsqueeze_(0)\n",
    "        return D\n",
    "\n",
    "\n",
    "class D_SUM_CALC(torch.nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cuda\")):\n",
    "        super(D_SUM_CALC, self).__init__()\n",
    "        self.device = device\n",
    "    def forward(self, input_D):\n",
    "        if list(input_D.shape)[0] > 1:\n",
    "            print('Warning - expected batch size 1')\n",
    "        D = input_D.squeeze(0)\n",
    "        N = list(D.shape)[0]\n",
    "        D_sum = torch.zeros(N,N, device=self.device)\n",
    "        # diagonal\n",
    "        for ii in range(N):\n",
    "            D_sum[ii,ii] = D[ii,ii]\n",
    "        # second diagonal\n",
    "        for ii in range(0, N-1):\n",
    "            D_sum[ii, ii+1] = D[ii:ii+1+1, ii:ii+1+1].sum()\n",
    "            D_sum[ii+1, ii] = D[ii, ii+1]\n",
    "        # rest\n",
    "        for oo in range(2, N):\n",
    "            for ii in range(0, N - oo):\n",
    "                D_sum[ii, ii + oo] = D[ii, ii + oo] + D[ii + oo, ii] + D_sum[ii, ii + oo - 1] + D_sum[ii + 1, ii + oo] - D_sum[ii + 1, ii + oo - 1]\n",
    "                D_sum[ii + oo, ii] = D_sum[ii, ii + oo]\n",
    "        D_sum.unsqueeze_(0)\n",
    "        return D_sum\n",
    "\n",
    "class C_TABLE_ALL(torch.nn.Module):\n",
    "    def __init__(self, K, device=torch.device(\"cuda\")):\n",
    "        super(C_TABLE_ALL, self).__init__()\n",
    "        self.K = K\n",
    "        self.device = device\n",
    "    def forward(self, input_D_sum):\n",
    "        if list(input_D_sum.shape)[0] > 1:\n",
    "            print('Warning - expected batch size 1')\n",
    "        D_sum = input_D_sum.squeeze(0)\n",
    "        N = list(D_sum.shape)[0]\n",
    "        K = self.K\n",
    "        C = torch.zeros(N, K, device=self.device)\n",
    "        C_all = -1 * torch.ones(N, K, N, device=self.device)\n",
    "        the_softmin = torch.nn.Softmin(dim=0)\n",
    "        for nn in range(N):\n",
    "            C[nn, 0] = D_sum[nn, N-1]\n",
    "            C_all[nn, 0, N-1] = 1.0\n",
    "        for kk in range(1, K):\n",
    "            for nn in range(0, N - kk):\n",
    "                temp = torch.empty(N - kk - nn, device=self.device)\n",
    "                for ii in range(nn, N - kk):\n",
    "                    temp[ii-nn] = D_sum[nn, ii] + C[ii + 1, kk - 1]\n",
    "                C_all[nn, kk, nn:N-kk] = the_softmin(temp)\n",
    "                C[nn, kk] = torch.min(temp)\n",
    "        C.unsqueeze_(0)\n",
    "        C_all.unsqueeze_(0)\n",
    "        return C, C_all\n",
    "\n",
    "class OSG_C(torch.nn.Module):\n",
    "    def __init__(self, feature_sizes, K_max=30, BN=False, DO=0.0, dist_type='EMBEDDING', dist_metric='cosine', device=torch.device(\"cuda\")):\n",
    "        super(OSG_C, self).__init__()\n",
    "        self.feature_sizes = feature_sizes\n",
    "        self.K_max = K_max\n",
    "        self.DIST_FUNC = DIST(feature_sizes,BN,DO,dist_type,dist_metric, device)\n",
    "        self.D_SUM_CALC = D_SUM_CALC(device)\n",
    "        self.C_TABLE_ALL = C_TABLE_ALL(K_max, device)\n",
    "        self.device = device\n",
    "        self.state_dict() # 내가 추가해줌\n",
    "\n",
    "    def forward(self, x): # input tensor 'x'를 인자로 받음\n",
    "        T_list = list()\n",
    "        if len(x.shape) == 2: # x의 shape이 2개의 차원으로 이루어져 있다면,\n",
    "            x.unsqueeze_(0) # 하나의 샘플만 있는 경우이므로 배치 차원을 추가하기 위해 차원을 확장해준다.\n",
    "        for x_input in x.unbind(): # x는 루프를 통해 반복된다.\n",
    "            x_input = torch.masked_select(x_input, ~torch.isnan(x_input)).view(1, -1, x_input.shape[1])\n",
    "            D = self.DIST_FUNC(x_input) # 거리 매트릭을 계산\n",
    "            D_sum = self.D_SUM_CALC(D) # 거리 합 계산\n",
    "            __, C_all = self.C_TABLE_ALL(D_sum) # 0보다 큰 값들을 선택해 평균을 계산, T_PRED_ALL에 저장\n",
    "            T_pred_all = torch.zeros(C_all.shape[3], device=self.device)\n",
    "            for ind in range(C_all.shape[3]):\n",
    "                T_pred_all[ind] = torch.masked_select(C_all[0, :, :, ind], C_all[0, :, :, ind].ge(0)).mean()\n",
    "            the_padding = torch.nn.modules.padding.ConstantPad1d((0, x.shape[1] - T_pred_all.shape[0]), float('nan')) # 패딩 추가\n",
    "            T_list.append(the_padding(T_pred_all)) # 패딩된 t_pred_all을 추가\n",
    "        T_out = torch.stack(T_list)\n",
    "\n",
    "        return T_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#osg_vsd_train.py\n",
    "\n",
    "import torch\n",
    "#import OSG_VSD as OSG\n",
    "#import my_Tloss\n",
    "#import OSG_C\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=300)\n",
    "#import OSG_VSD_DATASET\n",
    "#import OptimalSequentialGrouping\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 더 학습시키고 싶으면, stop_param 값을 낮춰주면 됨.\n",
    "def CLossTest(data_folder_path=\"D:\\Pycharm project\\LearnableOSG\\h5\", modality= 'visual', num_iters=1, stop_param=0.75):\n",
    "    if modality == 'visual':\n",
    "        path_to_h5 = data_folder_path + '/h5_visual/'\n",
    "        d, K_max = 2048, 5\n",
    "        BN = True\n",
    "        DO = 0.0\n",
    "        dist_metric = 'cosine'\n",
    "        dist_type = 'EMBEDDING'\n",
    "        feature_sizes = [d, 3000, 3000, 1000, 100]\n",
    "        learning_rate = 0.005\n",
    "        weight_decay = 0  # 1e-2\n",
    "    elif modality == 'audio':\n",
    "        path_to_h5 = data_folder_path + 'h5_audio/'\n",
    "        d, K_max = 128, 5\n",
    "        BN = True\n",
    "        DO = 0.0\n",
    "        dist_metric = 'cosine'\n",
    "        dist_type = 'EMBEDDING'\n",
    "        feature_sizes = [d, 200, 200, 100, 20]\n",
    "        learning_rate = 0.005\n",
    "        weight_decay = 0  # 1e-2\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    vsd_dataset = OSG_VSD_DATASET(path_to_h5=path_to_h5, device=device)\n",
    "\n",
    "    vsd_dataloader = torch.utils.data.DataLoader(vsd_dataset, collate_fn=my_collate)\n",
    "\n",
    "    OSG_model = OSG_C(feature_sizes, K_max=K_max, BN=BN, DO=DO, dist_type=dist_type, dist_metric=dist_metric,\n",
    "                          device=device)\n",
    "\n",
    "    OSG_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(OSG_model.parameters(), lr=learning_rate, weight_decay=weight_decay) # 모델의 파라미터를 최적화\n",
    "\n",
    "    print('Starting')\n",
    "    print('network', feature_sizes, 'dist_type', dist_type, 'dist_metric', dist_metric, 'stop_param', stop_param,\n",
    "          'modality', modality)\n",
    "\n",
    "    first_loss = 0\n",
    "    for iteration in tqdm(range(num_iters)):\n",
    "        optimizer.zero_grad()\n",
    "        all_loss = 0\n",
    "\n",
    "        for a_batch in tqdm(vsd_dataloader, leave=False):\n",
    "            x, t = a_batch\n",
    "            T_pred = OSG_model(x.to(device))\n",
    "            loss = my_Tloss(T_pred.to(device), t.to(device), device=device)\n",
    "            all_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "        if iteration == 0:\n",
    "            first_loss = all_loss\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        OSG_np = OptimalSequentialGrouping() # OSG 모델의 출력값을 기반으로 클러스터링 수행\n",
    "\n",
    "        F_trn = 0\n",
    "        for an_index in range(len(vsd_dataset)):\n",
    "            x_orig, t_orig = vsd_dataset[an_index]\n",
    "            t = t_orig.cpu().numpy()\n",
    "            D_temp = OSG_model.DIST_FUNC(x_orig.unsqueeze(0))\n",
    "            D_new = D_temp.squeeze(0).cpu().detach().numpy()\n",
    "            boundaries_new = OSG_np.blockDivideDSum(D_new, t.size)\n",
    "            F_temp, _, _ = OSG_np.FCO(boundaries_new, t)\n",
    "            F_trn += F_temp\n",
    "\n",
    "        print('Iteration ' + str(iteration+1) + ', loss: ' + str(all_loss) + ', F-score: ' + str(F_trn)) # F-Score 품질\n",
    "\n",
    "        if all_loss < stop_param * first_loss:\n",
    "            break\n",
    "\n",
    "    print('Finished')\n",
    "    torch.save(OSG_model.state_dict(), 'model_param7.pth')  # model.pth 저장해주기 위한 코드\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CLossTest(num_iters=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_loader(inference).py\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import re\n",
    "from torch._six import container_abcs, string_classes, int_classes\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
    "\n",
    "\n",
    "\n",
    "class OSG_VSD_DATASET(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_h5, device):\n",
    "        self.path_to_h5 = path_to_h5 # h5_new/h5_visual 경로로 설정\n",
    "        self.device = device\n",
    "        self.h5_files = [f.split('.hdf5')[0] for f in os.listdir(path_to_h5) if f.endswith('.hdf5')] # 폴더 안에 있는 hdf5들의 리스트를 저장\n",
    "        print(\"파일 목록:\",self.h5_files, \"파일 개수\", len(self.h5_files))\n",
    "        self.num_of_h5 = len(self.h5_files) # 몇개씩 받아올거냐\n",
    "\n",
    "    def __len__(self):\n",
    "        # Implement the logic to determine the number of samples in the dataset\n",
    "        return self.num_of_h5\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        the_file = h5py.File(self.path_to_h5 + self.h5_files[idx] + '.hdf5', 'r')\n",
    "        x_data = torch.tensor(the_file['x'], dtype=torch.float, device=self.device)\n",
    "\n",
    "        if x_data.shape[0] <= 1:\n",
    "            return None  # None 값을 반환\n",
    "        return x_data\n",
    "\n",
    "\n",
    "def my_collate_old(batch):\n",
    "    data = [item['x'] for item in batch]\n",
    "    target = [item['t'] for item in batch]\n",
    "    return data\n",
    "    #return [data, target]\n",
    "\n",
    "\n",
    "my_collate_err_msg_format = (\n",
    "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
    "\"dicts or lists; found {}\")\n",
    "\n",
    "def my_collate(batch):\n",
    "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
    "\n",
    "    elem = batch[0]\n",
    "    elem_type = type(elem)\n",
    "\n",
    "    if isinstance(elem, torch.Tensor):\n",
    "        out = None\n",
    "        max_length = max([x.shape[0] for x in batch])\n",
    "        numel = max_length*len(batch)\n",
    "        storage = elem.storage()._new_shared(numel)\n",
    "        out = elem.new(storage)\n",
    "        return torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=-1)\n",
    "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
    "            and elem_type.__name__ != 'string_':\n",
    "        elem = batch[0]\n",
    "        if elem_type.__name__ == 'ndarray':\n",
    "            # array of string classes and object\n",
    "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
    "                raise TypeError(my_collate_err_msg_format.format(elem.dtype))\n",
    "\n",
    "            return my_collate([torch.as_tensor(b) for b in batch])\n",
    "        elif elem.shape == ():  # scalars\n",
    "            return torch.as_tensor(batch)\n",
    "    elif isinstance(elem, float):\n",
    "        return torch.tensor(batch, dtype=torch.float64)\n",
    "    elif isinstance(elem, int_classes):\n",
    "        return torch.tensor(batch)\n",
    "    elif isinstance(elem, string_classes):\n",
    "        return batch\n",
    "    elif isinstance(elem, container_abcs.Mapping):\n",
    "        return {key: my_collate([d[key] for d in batch]) for key in elem}\n",
    "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
    "        return elem_type(*(my_collate(samples) for samples in zip(*batch)))\n",
    "    elif isinstance(elem, container_abcs.Sequence):\n",
    "        transposed = zip(*batch)\n",
    "        return [my_collate(samples) for samples in transposed]\n",
    "\n",
    "    raise TypeError(my_collate_err_msg_format.format(elem_type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inference.py\n",
    "import h5py\n",
    "import torch\n",
    "#import OSG_VSD as OSG\n",
    "import numpy as np\n",
    "np.set_printoptions(linewidth=300)\n",
    "#import osg_vsd_dataset_test\n",
    "#import OptimalSequentialGrouping\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "def inference(data_folder_path='D:\\Pycharm project\\LearnableOSG\\h5\\youtube_h5_x/', modality='visual', model_path='model.pth', result_path='result', num_iters=1, stop_param=0.75):\n",
    "    if modality == 'visual':\n",
    "        path_to_h5 = data_folder_path #+ 'h5_visual/'\n",
    "        h5_files = [f.split('.hdf5')[0] for f in os.listdir(path_to_h5) if f.endswith('.hdf5')]\n",
    "        print(\"파일 개수\", len(h5_files))\n",
    "        d, K_max = 2048, 5\n",
    "        BN = True\n",
    "        DO = 0.0\n",
    "        dist_metric = 'cosine'\n",
    "        dist_type = 'EMBEDDING'\n",
    "        feature_sizes = [d, 3000, 3000, 1000, 100]\n",
    "        learning_rate = 0.005\n",
    "        weight_decay = 0  # 1e-2\n",
    "#    elif modality == 'audio':\n",
    "#        path_to_h5 = data_folder_path + 'h5_audio/'\n",
    "#        d, K_max = 128, 5\n",
    "#        BN = True\n",
    "#        DO = 0.0\n",
    "#        dist_metric = 'cosine'\n",
    "#        dist_type = 'EMBEDDING'\n",
    "#        feature_sizes = [d, 200, 200, 100, 20]\n",
    "#        learning_rate = 0.005\n",
    "#        weight_decay = 0  # 1e-2\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    vsd_dataset = OSG_VSD_DATASET(path_to_h5=path_to_h5, device=device)\n",
    "\n",
    "    vsd_dataloader = torch.utils.data.DataLoader(vsd_dataset, collate_fn=my_collate)\n",
    "\n",
    "    OSG_model = OSG_C(feature_sizes, K_max=K_max, BN=BN, DO=DO, dist_type=dist_type, dist_metric=dist_metric, device=device).to(device) # 모델 초기화\n",
    "\n",
    "    checkpoint = torch.load('model_param7.pth',map_location='cuda:0')\n",
    "    OSG_model.load_state_dict(checkpoint)\n",
    "\n",
    "    #OSG_model.to(device)\n",
    "\n",
    "\n",
    "    # optimizer = torch.optim.Adam(OSG_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    print('Starting')\n",
    "    print('network', feature_sizes, 'dist_type', dist_type, 'dist_metric', dist_metric, 'stop_param', stop_param, 'modality', modality)\n",
    "\n",
    "    first_loss = 0\n",
    "\n",
    "    # 경계를 저장할 리스트\n",
    "    all_boundaries = []\n",
    "\n",
    "\n",
    "\n",
    "    # for iteration in tqdm(range(num_iters)):\n",
    "    #     optimizer.zero_grad()\n",
    "    #     #all_loss = 0\n",
    "    # \n",
    "    #     for a_batch in tqdm(vsd_dataloader, leave=False):\n",
    "    #         x = a_batch\n",
    "    #         #x, _ = a_batch\n",
    "    #         # T_pred = OSG_model(x.to(device)) # OSG Model에 배치를 입력하여, T_pred 값을 예측한다.\n",
    "    #         #print(T_pred)\n",
    "    #         #loss = OSG.my_Tloss(T_pred.to(device), t.to(device), device=device) # loss 값 계산\n",
    "    #         #all_loss += loss.item()\n",
    "    #         #loss.backward() # loss를 역전파하고 모델의 파라미터를 업데이트\n",
    "    # \n",
    "    # \n",
    "    #     #if iteration == 0:\n",
    "    #         #first_loss = all_loss\n",
    "    # \n",
    "    #     optimizer.step()\n",
    "    # \n",
    "    OSG_np = OptimalSequentialGrouping()\n",
    "    #     # print(f\"OSG_NP : {OSG_np}\") # 얘가 원하는 결과값\n",
    "    # \n",
    "    #     F_trn = 0\n",
    "    for an_index in range(len(vsd_dataset)):\n",
    "        print(f\"index : {an_index}\")\n",
    "\n",
    "        #x_orig, t_orig = vsd_dataset[an_index]\n",
    "        x_orig = vsd_dataset[an_index]\n",
    "        if x_orig == None:\n",
    "            continue\n",
    "        x_orig = x_orig.to(device) # x 값\n",
    "\n",
    "        #t = t_orig.cpu().numpy()\n",
    "        D_temp = OSG_model.DIST_FUNC(x_orig.unsqueeze(0)) # x_orig을 입력값으로 받아서 거리행렬 D를 만들어\n",
    "        D_new = D_temp.squeeze(0).cpu().detach().numpy()\n",
    "        # D_temp = D_temp.to(device)\n",
    "        #\n",
    "        # D_temp_np = D_temp.cpu().detach().numpy()\n",
    "        K_pred = OSG_np.estimate_num_scenes(D_new,D_new)\n",
    "        #K_pred = K_pred.to(device)\n",
    "        print(f\"K_predicted:{K_pred}\")\n",
    "        boundaries_new = OSG_np.blockDivideDSum(D_new, K_pred) # t.size 값(= 장면의 수)를 받아서 경계를 출력한다.\n",
    "        t_data=boundaries_new # t dataset 생성\n",
    "        print(f\"BOUNDARY_{an_index} : {boundaries_new}\")\n",
    "        #F_temp, _, _ = OSG_np.FCO(boundaries_new, t)\n",
    "        #F_trn += F_temp\n",
    "\n",
    "        # 경계와 씬 번호를 리스트에 추가\n",
    "        #all_boundaries.append((an_index, boundaries_new))\n",
    "        file_name = os.path.join(path_to_h5, f'{h5_files[an_index]}.hdf5')\n",
    "        print(\"inference file name:\", file_name)\n",
    "        with h5py.File(file_name, 'a') as h5_file:\n",
    "            #h5_file.create_dataset('t', data=t_data)\n",
    "            if 't' in h5_file.keys():\n",
    "                del h5_file['t']\n",
    "            h5_file.create_dataset('t', data=t_data)\n",
    "\n",
    "    #print('Iteration ' + str(iteration) + ', F-score: ' + str(F_trn))\n",
    "\n",
    "    #if all_loss < stop_param * first_loss:\n",
    "    #        break\n",
    "\n",
    "    print('Finished')\n",
    "\n",
    "    # 'result' 폴더 생성\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    # 경계를 파일에 저장\n",
    "    #with open(os.path.join(result_path, 'boundaries_test.txt'), 'w') as f:\n",
    "    #    for scene_index, boundaries in all_boundaries:\n",
    "    #        f.write('Video ' + str(scene_index) + ', Boundaries: ' + ', '.join(str(boundary) for boundary in boundaries) + '\\n')\n",
    "\n",
    "    # 결과 파일 경로 설정\n",
    "#    with open(os.path.join(result_path, 'result.txt'), 'w') as f:\n",
    "#        f.write('F-score: ' + str(F_trn))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inference()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) scene detection 후, raw video -> scene 별로 자르기\n",
    "- mmv model video input 전처리과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import re\n",
    "\n",
    "class VideoSceneAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.vid = {}\n",
    "\n",
    "    def read_shot_file(self, shot_filepath): # shot.txt 파일올 읽어들여서 shot, mid, frame_cnt 출력\n",
    "        # print(\"def : read_shot_file\", shot_filepath)\n",
    "        shot = pd.read_csv(shot_filepath, header=None, delimiter='\\t')\n",
    "        shot['mid'] = round((shot[0] + shot[1]) / 2).astype(int)\n",
    "        shot['frame_cnt'] = shot[1] - shot[0]\n",
    "        mid = round((shot[0] + shot[1]) / 2).astype(int).tolist()\n",
    "        frame_cnt = (shot[1] - shot[0]).tolist()\n",
    "        return shot, mid, frame_cnt\n",
    "\n",
    "    def read_hdf5_file(self, h5_filepath):\n",
    "        # print(h5_filepath)\n",
    "        # return [2,3,4,5]\n",
    "    # h5 파일 읽어오기 -> scene_boundary\n",
    "        file = h5py.File(h5_filepath, 'r')\n",
    "        dataset = file['t']\n",
    "        scene_boundary = dataset[()].astype(int) # 데이터 셋을 가져온 후, int 형태로 변환\n",
    "        file.close()\n",
    "        #print(type(scene_boundary))\n",
    "        #print(scene_boundary.shape)\n",
    "        #print(scene_boundary)\n",
    "        return scene_boundary # scene_boundary를 반환\n",
    "\n",
    "\n",
    "    def frame_in_scene(self, scene_boundary, frame_cnt): # scene 안에 frame 수 계산\n",
    "        n = len(scene_boundary) # n = scene의 개수\n",
    "        #print(\"scene의 개수\",n)\n",
    "        cnt = {}\n",
    "\n",
    "        for i in range(n): # 한 scene을 돌면서\n",
    "            total_sum = 0\n",
    "\n",
    "            try:\n",
    "                if i == 0: # 처음 돌면,\n",
    "                    r = range(0, scene_boundary[i])\n",
    "                else:\n",
    "                    r = range(scene_boundary[i-1], scene_boundary[i])\n",
    "\n",
    "                for j in r:\n",
    "                    total_sum += frame_cnt[j] # 프레임의 총 갯수\n",
    "\n",
    "                cnt['scene'+str(i)] = total_sum # 씬마다 씬에 해당하는 총 프레임 수 계산\n",
    "            except IndexError:\n",
    "                print(f\"IndexError occurred for scene boundary {i}, skipping...\")\n",
    "\n",
    "        return cnt # 딕셔너리 형태 {'scene i': frame의 총 개수}\n",
    "\n",
    "    def analyze_video_scene(self, shot_filepath, h5_filepath):\n",
    "        shot_data, mid, frame_cnt = self.read_shot_file(shot_filepath)\n",
    "        scene_boundary = self.read_hdf5_file(h5_filepath)\n",
    "\n",
    "        shot_filename = os.path.basename(shot_filepath).split(\"\\\\\")[-1].replace(\"_shots.txt\", \"\") # 파일 이름 추출\n",
    "        #print(shot_filename)\n",
    "        self.vid[shot_filename] = self.frame_in_scene(scene_boundary, frame_cnt)\n",
    "\n",
    "    def get_video_scene(self):\n",
    "        return self.vid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = VideoSceneAnalyzer()\n",
    "\n",
    "base_path = r\"D:\\youtube_data\"\n",
    "\n",
    "shot_txt_files = [f for f in os.listdir(base_path) if f.endswith('.txt')]\n",
    "hdf5_files = [f for f in os.listdir(base_path + \"\\\\h5_1\") if f.endswith('.hdf5')]\n",
    "\n",
    "for idx in range(len(shot_txt_files)):\n",
    "    shot_txt_path = os.path.join(base_path+ \"\\\\\" ,shot_txt_files[idx])\n",
    "    hdf5_path = os.path.join(base_path + \"\\\\h5_1\", hdf5_files[idx])\n",
    "    analyzer.analyze_video_scene(shot_txt_path, hdf5_path)\n",
    "\n",
    "video_scenes = analyzer.get_video_scene()\n",
    "print(video_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(video_scenes).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\youtube_data\\dataframe\\model_param7.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"D:\\youtube_data\\dataframe\\model_dropout_weight_decay2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비디오 + 오디오 -> scene 별로 저장\n",
    "import subprocess\n",
    "\n",
    "baseUrl = r\"D:\\qvhilights_videos\\videos\"\n",
    "\n",
    "for i in range(len(df['Unnamed: 0'])):\n",
    "    video_name = df.iloc[i][0]\n",
    "    path = baseUrl + \"\\\\\" + video_name + \".mp4\" # input video path, mp4만 있다고 가정\n",
    "    frame_list = []\n",
    "    for j in range(df.shape[1]):\n",
    "        if not(pd.isna(df.iloc[i][j])):\n",
    "            frame_list.append(df.iloc[i][j])\n",
    "    print(f\"frame_list {i}\", frame_list)\n",
    "\n",
    "    video_capture = cv2.VideoCapture(path) # input video capture\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS) # 현재 동영상의 fps 가져오기\n",
    "    video_capture.release()\n",
    "\n",
    "    output_folder = os.path.join(r\"D:\\test\", video_name) # output video path\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    start_frame = 0\n",
    "    i_current_frame = 0 # 현재 프레임 순서를 나타내는 변수\n",
    "    for frame in frame_list[1:]:\n",
    "        output_path = os.path.join(output_folder, f\"{video_name}_{start_frame}.mp4\")\n",
    "        last_frame = int(frame) + i_current_frame - 1\n",
    "        print(\"frame:\", frame)\n",
    "\n",
    "        command = [\n",
    "            \"ffmpeg\",\n",
    "            \"-ss\", str(start_frame/fps),\n",
    "            \"-i\", path,\n",
    "            \"-t\", str(int(frame)/fps),\n",
    "            \"-c:v\", \"libx264\",\n",
    "            \"-c:a\", \"aac\",\n",
    "            \"-strict\", \"-2\",\n",
    "            output_path\n",
    "        ]\n",
    "        subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "        print(\"start frame:\", start_frame, \"last frame:\", last_frame)\n",
    "        start_frame = last_frame + 1\n",
    "        print(\"씬 별 총 프레임 개수:\", i_current_frame)\n",
    "\n",
    "    i_current_frame = 0 #초기화\n",
    "\n",
    "print(\"최종 끝\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
